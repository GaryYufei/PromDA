batch_size: 16
gradient_accumulation_steps: 2
checkpoint_every_step: 5000
num_training_steps: 5000
tokenizer_type: t5-large
lm_type: t5-large
prefix_length: 5
prefix_set_number: 2
sample_num: 1
label_path: NLU_training_dataset/conll03/labels.txt
dev_path: NLU_training_dataset/conll03/valid_whole.txt
test_path: NLU_training_dataset/conll03/test_whole.txt                              
