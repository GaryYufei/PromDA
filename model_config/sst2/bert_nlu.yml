batch_size: 8
adam_epsilon: 1e-8
learning_rate: 5e-5
weight_decay: 5e-6
checkpoint_every_step: 400
num_training_steps: 4000
max_epoch: 30
max_length: 100
warmup_step: 20
lm_type: bert-base-uncased
label_path: NLU_training_dataset/sst2/labels.txt
dev_path: NLU_training_dataset/sst2/valid_whole.txt
test_path: NLU_training_dataset/sst2/test_whole.txt
enable_sentence_classification: True
